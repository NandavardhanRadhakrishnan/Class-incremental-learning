{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "def create_mnist_data(class_splits, split_index, csv_file=\"mnist.csv\", image_shape=(1, 28, 28)):\n",
        "    \"\"\"\n",
        "    Creates MNIST dataset for a specific split.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "    all_classes = sorted(df['label'].unique())\n",
        "\n",
        "    start_class = sum(class_splits[:split_index])\n",
        "    end_class = start_class + class_splits[split_index]\n",
        "    valid_classes = all_classes[start_class:end_class]\n",
        "\n",
        "    filtered_df = df[df['label'].isin(valid_classes)]\n",
        "\n",
        "    X = filtered_df.drop(columns=['label']).values\n",
        "    y = filtered_df['label'].values\n",
        "\n",
        "    label_mapping = {cls: i for i, cls in enumerate(valid_classes)}\n",
        "    y = np.array([label_mapping[label] for label in y])\n",
        "\n",
        "    X = torch.tensor(X / 255.0, dtype=torch.float32)\n",
        "    X = X.view(-1, *image_shape)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    return TensorDataset(X, y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_group_data(class_splits, csv_file=\"mnist.csv\", image_shape=(1, 28, 28)):\n",
        "    \"\"\"\n",
        "    Creates MNIST dataset for the grouping model (e.g., maps different splits to groups).\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "\n",
        "    def relabel_digit(digit):\n",
        "        for i, split in enumerate(class_splits):\n",
        "            if digit < sum(class_splits[:i + 1]):\n",
        "                return i\n",
        "        return len(class_splits) - 1\n",
        "\n",
        "    df['label'] = df['label'].apply(relabel_digit)\n",
        "\n",
        "    X = df.drop(columns=['label']).values\n",
        "    y = df['label'].values\n",
        "\n",
        "    X = torch.tensor(X / 255.0, dtype=torch.float32)\n",
        "    X = X.view(-1, *image_shape)\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    return TensorDataset(X, y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, 2, 2)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, data_loader, criterion, optimizer, epochs, device):\n",
        "    \"\"\"\n",
        "    Trains a given model.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        with tqdm(total=len(data_loader), desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\") as pbar:\n",
        "            for X_batch, y_batch in data_loader:\n",
        "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                pbar.set_postfix(loss=total_loss / (pbar.n + 1))\n",
        "                pbar.update(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "def inference_with_grouping(models, model_group, combined_loader, class_splits, index=None):\n",
        "    \"\"\"\n",
        "    Performs inference using multiple models based on a grouping model.\n",
        "    \"\"\"\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_group.eval()\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(combined_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            if len(images.shape) == 3:\n",
        "                images = images.unsqueeze(1)\n",
        "\n",
        "            # If index is provided, infer only that index\n",
        "            if index is not None:\n",
        "                images = images[index].unsqueeze(0)\n",
        "                labels = labels[index].unsqueeze(0)\n",
        "\n",
        "            # Determine which model to use\n",
        "            group_logits = model_group(images)\n",
        "            group_pred = torch.argmax(group_logits, dim=1)\n",
        "\n",
        "            final_preds = []\n",
        "            for i in range(images.shape[0]):\n",
        "                model_idx = group_pred[i].item()\n",
        "                model = models[model_idx]\n",
        "                logits = model(images[i].unsqueeze(0))\n",
        "                pred = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "                # Offset to match global label space\n",
        "                offset = sum(class_splits[:model_idx])\n",
        "                final_preds.append(pred + offset)\n",
        "\n",
        "            all_preds.extend(final_preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            if batch_idx % 2000 == 0 or index is not None:\n",
        "                print(f\"Batch {batch_idx} Predictions:\")\n",
        "                print(f\"Predicted: {final_preds}\")\n",
        "                print(f\"True:      {labels.cpu().numpy()}\")\n",
        "\n",
        "            if index is not None:\n",
        "                break\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\nOverall Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds, labels=list(range(sum(class_splits))))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=list(range(sum(class_splits))),\n",
        "                yticklabels=list(range(sum(class_splits))))\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "    print(cm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- Main Script ----\n",
        "class_splits = [2, 4, 2, 2]  # Example class split\n",
        "image_shape = (1, 28, 28)\n",
        "batch_size = 64\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create datasets and models\n",
        "models = []\n",
        "data_loaders = []\n",
        "for i, num_classes in enumerate(class_splits):\n",
        "    dataset = create_mnist_data(class_splits, i, image_shape=image_shape)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    data_loaders.append(loader)\n",
        "\n",
        "    model = SimpleCNN(num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_model(model, loader, criterion, optimizer, epochs=3, device=device)\n",
        "    models.append(model)\n",
        "\n",
        "# Create and train grouping model\n",
        "group_data = create_group_data(class_splits, image_shape=image_shape)\n",
        "group_loader = DataLoader(group_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model_group = SimpleCNN(len(class_splits)).to(device)\n",
        "optimizer_group = optim.Adam(model_group.parameters(), lr=0.001)\n",
        "criterion_group = nn.CrossEntropyLoss()\n",
        "train_model(model_group, group_loader, criterion_group, optimizer_group, epochs=3, device=device)\n",
        "\n",
        "# Perform inference\n",
        "combined_data = create_mnist_data([sum(class_splits)], 0, image_shape=image_shape)\n",
        "combined_loader = DataLoader(combined_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "inference_with_grouping(models, model_group, combined_loader, class_splits)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMe4ZTY+rcQCosZGvxS/RpW",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
